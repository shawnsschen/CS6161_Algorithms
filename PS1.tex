%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[titlepage, paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{listings}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template


\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{University of Virginia} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CS 6161 Algorithms \\
\huge Problem Set 1 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Shawn (Shuoshuo) Chen\\sc7cq@virginia.edu\\Group partner: Rolph Recto\\ rjr7je@virginia.edu} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section*{Problem 1}
\textbf{(a).}
Assuming the value of ${x_1,x_2, ... ,x_n}$ is pick at random independently. Then the probability of getting true
and false is equal, where $p_{true} = p_{false} = 0.5$. For a clause $C_i$, the probability of evaluating true is
$E[p_{true}] = 1-2^{-d}$. And there are $m$ clauses in $\phi$, so the number of satified clauses is
$E[N_{SAT}] = (1-2^{-d}) * m$, which indicates there is an assignment satisfying at least $(1-2^{-d})m$ clauses.
\\

\textbf{(b).}
\\

\textbf{(c).}
\\


%----------------------------------------------------------------------------------------
%	PROBLEM 3
%----------------------------------------------------------------------------------------

\section*{Problem 3}
\textbf{(a).}
According to the definition of expectation, for a random variable $X: \Omega \to \mathbb{Z}$, there is:
\begin{align*} 
\begin{split}
\mathbb{E}[X] &= \sum_{\omega \in \Omega} X(\omega)Pr[\omega] \\
\end{split}					
\end{align*}
So for the claim:
\begin{align*} 
\begin{split}
\mathbb{E}[\sum_{i} X_i] &= \sum_{\omega \in \Omega} \sum_{i} X_i(\omega)Pr[\omega] \\
&= \sum_{i} \sum_{\omega \in \Omega} X_i(\omega)Pr[\omega] \\
&= \sum_{i} \mathbb{E}[X_i]
\end{split}					
\end{align*}
\\

\textbf{(b).}
Imagine that $\{X_i\}$ is somehow controlled by an invisible hand such that
$\forall X_i \in \{X_i\}$, it becomes the same value as others.
In other words, $X_1=X_2= ... =X_i$ at the same time.
Now assume $X_i$ is either $0$ or $1$. Essentially, $X_1X_2...X_i$ is either $0$ or $1$.
Thus, there is:
\begin{align*} 
\begin{split}
\mathbb{E}[\prod X_i] &= \mathbb{E}[X_1X_2...X_i] = \frac{1}{2} \\
&\neq \prod_i \mathbb{E}[X_i] = \prod_i \frac{1}{2} = \frac{1}{2^i}
\end{split}					
\end{align*}
\\

\textbf{(c).}
If $\{X_i\}$ is independent, $Pr[X_1X_2...X_i]=Pr[X_1]Pr[X_2]...Pr[X_i]$.
Then,
\begin{align*} 
\begin{split}
\mathbb{E}[\prod X_i] &= \mathbb{E}[X_1X_2...X_i] \\
&= \sum_{X_1X_2...X_i} X_1X_2...X_iPr[X_1=x_1,X_2=x_2, ... ,X_i=x_i] \\
&= \sum_{X_1}\sum_{X_2}...\sum_{X_i} x_1x_2...x_iPr[X_1=x_1,X_2=x_2, ... ,X_i=x_i] \\
&= \sum_{X_1}\sum_{X_2}...\sum_{X_i} x_1x_2...x_iPr[X_1=x_1]Pr[X_2=x_2]...Pr[X_i=x_i] \\
&= \sum_{X_1}x_1Pr[X_1=x_1]\sum_{X_2}x_2Pr[X_2=x_2]...\sum_{X_i}x_iPr[X_i=x_i] \\
&= \mathbb{E}[X_1]\mathbb{E}[X_2]...\mathbb{E}[X_i] \\
&= \prod_i \mathbb{E}[X_i]
\end{split}					
\end{align*}
\\



%----------------------------------------------------------------------------------------
%	PROBLEM 4
%----------------------------------------------------------------------------------------

\section*{Problem 4}
\textbf{18.16 (a).}
According to $D = P(H) * E[D | H] + P(T) * E[D | T] = H * D + T * (H * D + T)$ where $D$ is the state of both the start point and end point of H and TH in a tree diagram, there is:
\begin{align*} 
\begin{split}
E[N_{TT}] &= 0.5 * E[N_{TT} | H] + 0.5 * E[N_{TT} | T] \\
&= 0.5 * (1 + N_{TT}) + 0.5 * [1 + E[N_{TT} | 2nd]] \\
&= 0.5 * (1 + N_{TT}) + 0.5 * [1 + 0.5 * 1 + 0.5 * (1 + N_{TT})]
\end{split}					
\end{align*}
Solve for $N_{TT}$ and we get $N_{TT} = 6$.
Note that if we get a head (H), it basically adds one toss to the expectation. On the other hand, if we get a tail (T) followed by a head (H), it adds one toss to $E[N_{T*}]$, where $E[N_{T*}]$ is a combination of TT and TH. If we reach TT, then that's the end, just add another toss. Otherewise we need to start over again, so just plug the same structure $1 + N_{TT}$ in.
\\
\textbf{18.16 (b).}
Similarly, there is $C = H * C + T * B$ where $B = H + T * B$ for the tree diagram. We can derive:
\begin{align*} 
\begin{split}
E[N_{TH}] &= 0.5 * ( 1 + E[N_{TH} | H] ) + 0.5 * E[N_{B} | T] \\
\end{split}					
\end{align*}
And for $E_{B}$, there is:
\begin{align*} 
\begin{split}
E[N_{B}] &= 0.5 * 1 + 0.5 * ( 1 + E[N_{B}] ) \\
\end{split}					
\end{align*}
Solve for $N_{B}$ we get $N_{B} = 2$, and plug it into the $N_{TH}$ equation we get $N_{TH} = 4$.
\\
\textbf{18.16 (c).}
Since TT and TH both have the same first toss T, and by probability, the chance of getting T or H in the second toss is equally half. So $P(TT) = P(TH) = 0.5$. Then we can get the expected profit:
\begin{align*} 
\begin{split}
E[Profit] &= 0.5 * 6 + 0.5 * (-5) \\
&= 0.5 \quad dollars
\end{split}					
\end{align*}
\\

\textbf{18.19}
We can come up with a tree diagram with 3 levels. And there is:
\begin{align*} 
\begin{split}
E[N_{TTH}] &= p * (1+N_{TTH}) + (1-p) * (1+E[N_C]) \quad where \\
E[N_C] &= p * (1+N_{TTH}) + (1-p) * (1+E[N_B]) \quad where \\
E[N_B] &= p * 1 + (1-p) * (1+E[N_B])
\end{split}					
\end{align*}
First solve for $E[N_B]$, we get $E[N_B] = \frac{1}{p}$. Then plug $E[N_B]$ into $E[N_C]$, we get
$E[N_C] = \frac{1}{p} + p * E[N_{TTH}]$. Finally, plug $E[N_C]$ into $E[N_{TTH}]$, we get $E[N_{TTH}] = \frac{1}{p(p-1)^2}$. \\
To minimize $E[N_{TTH}]$, we differentiate it with respect to $p$:
\begin{align*} 
\begin{split}
d(E[N_{TTH}])/dp &= \frac{1-3p}{(p-1)^3p^2} \\
\end{split}					
\end{align*}
So when $p=\frac{1}{3}$, $E[N_{TTH}]$ becomes minimum.
\\


%----------------------------------------------------------------------------------------
%	PROBLEM 5
%----------------------------------------------------------------------------------------

\section*{Problem 5}
\textbf{1.}
Let us use the same notation from the class, where $m$ is the number of edges in $G$.
Then we can see $c^* \leq m$. \\

Now, we randomly color each node with one of the three colors. Since the nodes are uniformly distributed and pairwise independent, the probability of getting each color is equally $\frac{1}{3}$. Then we can use the similar trick: \\
Let $Y_e$ be a random variable that indicates whether an edge $e$ is satified or not:
\begin{gather*}
Y_e =
\begin{cases}
1 \quad satisfied \\
0 \quad not\ satisfied \\
\end{cases}
\end{gather*}
It is easy to see that there are $3 \time 3 = 9$ combinations or coloring, where $3$ of them do not satisfy. So:
\begin{align*} 
\begin{split}
\mathbb{E}[Y_e]=\frac{2}{3}
\end{split}					
\end{align*}
And let $Y$ be a random variable indicating the total number of satisfied edges:
\begin{align*} 
\begin{split}
Y = \sum_{e \in [m]} Y_e
\end{split}					
\end{align*}
There is:
\begin{align*} 
\begin{split}
\mathbb{E}[Y] &= \mathbb{E}[\sum_{e \in [m]} Y_e] \\
&= \sum_{e \in [m]} \mathbb{E}[Y_e] \\
&= \frac{2}{3}m \geq \frac{2}{3}c^*
\end{split}					
\end{align*}
\\

\textbf{8.}
First, we just uniformly pick $k \in [n]$ nodes at random. And let
\begin{gather*}
Y_{uv} =
\begin{cases}
1 \quad exist\ an\ edge\ between\ (u,v) \\
0 \quad not\ exist \\
\end{cases}
\end{gather*}
Note that there will be $2m$ edges in total because (u,v) and (v,u) are doubly counted. \\
Then we can get:
\begin{align*} 
\begin{split}
\mathbb{E}[Y_{uv}] &= \frac{2m}{n(n-1)} \\
\end{split}					
\end{align*}
Note that u and v cannot be the same node, otherwise it is not an edge. \\
Then in $G[k]$, the expected number of edges is:
\begin{align*} 
\begin{split}
\mathbb{E}[G[k]_{edge}] &= \mathbb{E}[\sum_{k \in [n]} Y_[uv]] \\
&= \sum_{k \in [n]} \mathbb{E}[Y_{uv}] \\
&= \frac{k(k-1)m}{n(n-1)}
\end{split}					
\end{align*}
Now assume $e=\mathbb{E}[G[k]_{edge}]$, its upper bound is $e_m = \frac{k(k-1)}{2}$ (a full graph).
We can repeatedly run this randomized algorithm multiple times until we get the expected value of edges. The probability of getting $e$ in a single run is $p$. And the probability of finding $j$ edges is $p_j$.
So there is:
\begin{align*} 
\begin{split}
e &= \sum_{j}jp_j = \sum_{j \textless e}jp_j + \sum_{j \geq e}jp_j \\
&\textless \sum_{j \textless e}(e - \epsilon)p_j + \sum_{j \geq e}(e_m)p_j \\
&= (e-\epsilon)(1-p) + \frac{k(k-1)}{2}p \\
&\leq e - \epsilon + \frac{k(k-1)}{2}p
\end{split}					
\end{align*}
We can see that $p \geq \frac{2\epsilon}{k(k-1)}$. Since the randomized algorithm itself is polynomial time, and repeating it $\frac{1}{p}$ times is still polynomial. Thus, we can just run this algorithm $\frac{1}{p}$ times to get the expected edges.


\end{document}
